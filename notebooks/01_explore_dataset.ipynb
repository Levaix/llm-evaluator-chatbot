{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "\n",
    "This notebook explores the Q&A database used for the LLM evaluator chatbot.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The dataset contains question-answer pairs covering general machine learning concepts. Each entry has:\n",
    "- `question`: The question text\n",
    "- `answer`: The standard/reference answer\n",
    "\n",
    "This dataset serves as the knowledge base for evaluating student answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python-dotenv not installed. Using system environment variables only.\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file (if it exists)\n",
    "# This allows notebooks to use the same configuration as the main app\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  # Loads variables from .env file in project root\n",
    "    print(\"Environment variables loaded from .env file\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed. Using system environment variables only.\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not load .env file: {e}\")\n",
    "    print(\"Using system environment variables only.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Levin\\OneDrive\\Desktop\\DAI Assignment Part 2\n",
      "Project root: C:\\Users\\Levin\\OneDrive\\Desktop\\DAI Assignment Part 2\n",
      "Loading dataset from: data\\Q&A_db_practice.json\n",
      "Absolute path: C:\\Users\\Levin\\OneDrive\\Desktop\\DAI Assignment Part 2\\data\\Q&A_db_practice.json\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Find project root by looking for src/ directory\n",
    "current = Path.cwd()\n",
    "project_root = None\n",
    "\n",
    "# Check if we're in notebooks/ directory\n",
    "if current.name == 'notebooks':\n",
    "    project_root = current.parent\n",
    "else:\n",
    "    # Walk up the directory tree looking for src/ folder\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / 'src').exists() and (parent / 'src' / '__init__.py').exists():\n",
    "            project_root = parent\n",
    "            break\n",
    "    \n",
    "    # Fallback: assume current directory is project root if src/ exists here\n",
    "    if project_root is None and (current / 'src').exists():\n",
    "        project_root = current\n",
    "\n",
    "# If still not found, use current directory's parent\n",
    "if project_root is None:\n",
    "    project_root = current.parent if current.name == 'notebooks' else current\n",
    "\n",
    "# Change to project root directory so relative paths work correctly\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Add project root to path to import src modules\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "from src.data_loader import load_qa_dataset\n",
    "from src.config import DATA_PATH\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Loading dataset from: {DATA_PATH}\")\n",
    "# Resolve the path relative to project root\n",
    "data_path_absolute = (project_root / \"data\" / \"Q&A_db_practice.json\").resolve()\n",
    "print(f\"Absolute path: {data_path_absolute}\")\n",
    "print(f\"File exists: {data_path_absolute.exists()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_loader:Successfully loaded 150 question-answer pairs from c:\\Users\\Levin\\OneDrive\\Desktop\\DAI Assignment Part 2\\data\\Q&A_db_practice.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 150 questions\n",
      "\n",
      "Dataset shape: (150, 3)\n",
      "\n",
      "Columns: ['id', 'question', 'answer']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset (use absolute path to ensure it works)\n",
    "data_file = project_root / \"data\" / \"Q&A_db_practice.json\"\n",
    "df = load_qa_dataset(path=data_file)\n",
    "print(f\"Dataset loaded: {len(df)} questions\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "Total questions: 150\n",
      "\n",
      "Question length statistics (characters):\n",
      "count    150.000000\n",
      "mean      16.833333\n",
      "std        6.726105\n",
      "min        4.000000\n",
      "25%       12.250000\n",
      "50%       16.000000\n",
      "75%       20.000000\n",
      "max       38.000000\n",
      "Name: question, dtype: float64\n",
      "\n",
      "Answer length statistics (characters):\n",
      "count    150.000000\n",
      "mean     397.900000\n",
      "std      192.832916\n",
      "min      174.000000\n",
      "25%      274.000000\n",
      "50%      342.000000\n",
      "75%      425.000000\n",
      "max      927.000000\n",
      "Name: answer, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Display basic statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total questions: {len(df)}\")\n",
    "print(f\"\\nQuestion length statistics (characters):\")\n",
    "print(df['question'].str.len().describe())\n",
    "print(f\"\\nAnswer length statistics (characters):\")\n",
    "print(df['answer'].str.len().describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Questions (first 3):\n",
      "================================================================================\n",
      "\n",
      "Question ID: 0\n",
      "Question: Activation Function\n",
      "Answer (preview): An activation function is a mathematical function that transforms! each neuron’s aggregated input (pre‑activation) into its output signal by applying a non‑linear, usually differentiable mapping that ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question ID: 1\n",
      "Question: Anomaly Detection\n",
      "Answer (preview): Anomaly detection is a subfield of unsupervised learning that identifies data points whose feature patterns deviate significantly from the statistical regularities of a reference dataset, assuming ano...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question ID: 2\n",
      "Question: Area Under the Curve (AUC)\n",
      "Answer (preview): The Area Under the Curve (AUC) is a scalar performance metric for binary classifiers that is defined as the definite integral of the receiver operating characteristic (ROC) curve with respect to the f...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display sample entries\n",
    "print(\"Sample Questions (first 3):\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in df.head(3).iterrows():\n",
    "    print(f\"\\nQuestion ID: {row['id']}\")\n",
    "    print(f\"Question: {row['question']}\")\n",
    "    print(f\"Answer (preview): {row['answer'][:200]}...\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Usage\n",
    "\n",
    "This dataset is used in the Streamlit chatbot as follows:\n",
    "\n",
    "1. **Question Selection**: Questions are randomly selected from the dataset\n",
    "2. **Reference Answer**: The `answer` field serves as the \"gold standard\" for comparison\n",
    "3. **Evaluation**: Student answers are compared against the reference using:\n",
    "   - LLM-based evaluation (explanation + score)\n",
    "   - Automatic metrics (ROUGE-1, ROUGE-L)\n",
    "\n",
    "The dataset should contain diverse ML concepts to provide a comprehensive evaluation experience.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
